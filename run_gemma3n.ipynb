{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. SETUP AND INSTALLATION\n# -------------------------\nprint(\"Installing required packages (transformers from source)...\")\n!pip install -q timm --upgrade\n!pip install -q accelerate\n!pip install -q git+https://github.com/huggingface/transformers.git\n\nimport os\nimport time\nimport torch\nimport kagglehub\nfrom transformers import AutoTokenizer, AutoModelForCausalLM \nfrom typing import List\n\nprint(\"🔄 Configuring PyTorch backend for compatibility...\")\ntry:\n    import torch._dynamo\n    torch._dynamo.reset() \n    os.environ[\"TORCH_COMPILE_BACKEND\"] = \"eager\"\n    torch._dynamo.config.suppress_errors = True\nexcept ImportError:\n    print(\"⚠️ Could not import torch._dynamo. Skipping dynamo configuration.\")\n\nprint(\"✅ Packages installed successfully!\")\n\n\n# 2. MODEL CONFIGURATION\n# ----------------------\nprint(\"Downloading model files with kagglehub...\")\nMODEL_PATH = kagglehub.model_download(\"google/gemma-3n/transformers/gemma-3n-e2b-it\")\nprint(f\"✅ Model path set to: {MODEL_PATH}\")\n\n\n# 3. GENERAL-PURPOSE TRANSLATOR CLASS\n# -----------------------------------\nclass GemmaTranslator: \n    def __init__(self, model_path):\n        self.model_path = model_path\n        self.tokenizer = None\n        self.model = None\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self._initialize_model()\n\n    def _initialize_model(self):\n        \"\"\"Loads the Gemma model using AutoModelForCausalLM.\"\"\"\n        if not os.path.exists(self.model_path):\n            print(\"❌ Cannot initialize model: Path does not exist.\")\n            return\n\n        try:\n            print(\"🔄 Loading tokenizer...\")\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n            \n            print(\"🔄 Loading model... (This may take a moment)\")\n            self.model = AutoModelForCausalLM.from_pretrained(\n                self.model_path,\n                torch_dtype=torch.float16,\n            ).to(self.device)\n\n            print(\"✅ Translator initialized successfully!\")\n            if self.model:\n                  print(f\"📊 Model configured for device: {self.device}\")\n\n        except Exception as e:\n            print(f\"❌ Error initializing model: {e}\")\n            self.model = None\n            self.tokenizer = None\n            \n    # CHANGED: This method now accepts a `target_language` argument.\n    def translate(self, english_text: str, target_language: str) -> str:\n        \"\"\"\n        Translates English text into the specified target language.\n        \"\"\"\n        if not self.model or not self.tokenizer:\n            return \"❌ Model not initialized. Please check for errors above.\"\n\n        prompt = f\"\"\"Translate the following English text exactly into {target_language}. Provide only the {target_language} translation.\n\nEnglish: \"{english_text}\"\n{target_language}:\"\"\"\n\n        try:\n            start_time = time.time()\n            \n            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n            \n            # This is the key fix for GPU compatibility.\n            outputs = self.model.generate(**inputs, max_new_tokens=100, disable_compile=True)\n            \n            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            end_time = time.time()\n            \n            # The response parsing is now also dynamic.\n            if f\"{target_language}:\" in response:\n                translation = response.split(f\"{target_language}:\")[-1].strip()\n            else:\n                translation = response[len(prompt) - len(f'\\n{target_language}:'):].strip()\n            \n            translation = translation.replace('\"', '').strip()\n            \n            print(f\"⏱️  '{english_text[:30]}...' -> {target_language} in {end_time - start_time:.2f}s\")\n            return translation\n\n        except Exception as e:\n            return f\"❌ Translation error: {e}\"\n\n\n# 4. INITIALIZE AND TEST THE MULTILINGUAL TRANSLATOR\n# ----------------------------------------------------\nprint(\"\\n--- Initializing Translator ---\")\ntranslator = GemmaTranslator(MODEL_PATH)\nprint(\"-----------------------------\\n\")\n\nif translator.model:\n    # A dictionary of sentences to test the new multilingual capability.\n    test_cases = {\n        \"Where is the nearest library?\": \"Spanish\",\n        \"How much does this cost?\": \"German\",\n        \"I would like to order a coffee.\": \"French\",\n        \"This is a very powerful language model.\": \"Italian\",\n    }\n\n    print(\"--- Running Multilingual Translation Examples ---\")\n    for sentence, language in test_cases.items():\n        translation = translator.translate(sentence, language)\n        print(f\"🇺🇸 English: {sentence}\")\n        print(f\"🌍 {language}: {translation}\")\n        print(\"-\" * 40)\n        \n    print(\"\\n🎉 All tasks complete!\")\nelse:\n    print(\"Could not run translation examples because the model failed to initialize.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T23:40:19.885308Z","iopub.execute_input":"2025-06-28T23:40:19.886044Z","iopub.status.idle":"2025-06-28T23:41:25.639702Z","shell.execute_reply.started":"2025-06-28T23:40:19.886019Z","shell.execute_reply":"2025-06-28T23:41:25.638986Z"}},"outputs":[{"name":"stdout","text":"Installing required packages (transformers from source)...\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n🔄 Configuring PyTorch backend for compatibility...\n✅ Packages installed successfully!\nDownloading model files with kagglehub...\n✅ Model path set to: /kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1\n\n--- Initializing Translator ---\n🔄 Loading tokenizer...\n🔄 Loading model... (This may take a moment)\n","output_type":"stream"},{"name":"stderr","text":"2025-06-28 23:40:53.173460: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751154053.199897     149 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751154053.208074     149 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e68e3b4ad014bf8b7f25a1583ba4c06"}},"metadata":{}},{"name":"stdout","text":"✅ Translator initialized successfully!\n📊 Model configured for device: cuda\n-----------------------------\n\n--- Running Multilingual Translation Examples ---\n⏱️  'Where is the nearest library?...' -> Spanish in 13.12s\n🇺🇸 English: Where is the nearest library?\n🌍 Spanish: \n----------------------------------------\n⏱️  'How much does this cost?...' -> German in 0.89s\n🇺🇸 English: How much does this cost?\n🌍 German: Wie viel kostet das?\n----------------------------------------\n⏱️  'I would like to order a coffee...' -> French in 1.15s\n🇺🇸 English: I would like to order a coffee.\n🌍 French: Je voudrais commander un café.\n----------------------------------------\n⏱️  'This is a very powerful langua...' -> Italian in 1.42s\n🇺🇸 English: This is a very powerful language model.\n🌍 Italian: Questo è un modello linguistico molto potente.\n----------------------------------------\n\n🎉 All tasks complete!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print(\"🌍 Gemma Interactive Translator\")\nprint(\"---------------------------------\")\nprint(\"Enter the English text you want to translate.\")\nprint(\"Then, enter the target language (e.g., Spanish, German, Japanese).\")\nprint(\"Type 'quit' at any time to exit the translator.\")\nprint(\"---------------------------------\")\n\n# Start an infinite loop to keep the translator running\nwhile True:\n    # Get input text from the user\n    english_input = input(\"\\n🇬🇧 English text (or 'quit' to exit): \")\n    \n    # Check if the user wants to exit\n    if english_input.lower() == 'quit':\n        print(\"\\nExiting translator. Goodbye!\")\n        break\n        \n    # Get the target language from the user\n    language_input = input(f\"🌍 Target language for '{english_input[:50]}...': \")\n\n    # Check for exit condition again\n    if language_input.lower() == 'quit':\n        print(\"\\nExiting translator. Goodbye!\")\n        break\n        \n    print(\"\\n🔄 Translating...\")\n    \n    # Use the existing 'translator' object to perform the translation\n    translated_text = translator.translate(english_input, language_input)\n    \n    # Print the final translation\n    print(f\"\\n✅ Result in {language_input}:\")\n    print(translated_text)\n    print(\"---------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T23:42:48.104924Z","iopub.execute_input":"2025-06-28T23:42:48.106207Z","iopub.status.idle":"2025-06-28T23:43:17.991907Z","shell.execute_reply.started":"2025-06-28T23:42:48.106173Z","shell.execute_reply":"2025-06-28T23:43:17.991093Z"}},"outputs":[{"name":"stdout","text":"🌍 Gemma Interactive Translator\n---------------------------------\nEnter the English text you want to translate.\nThen, enter the target language (e.g., Spanish, German, Japanese).\nType 'quit' at any time to exit the translator.\n---------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n🇬🇧 English text (or 'quit' to exit):  How is the weather today\n🌍 Target language for 'How is the weather today...':  Japanese\n"},{"name":"stdout","text":"\n🔄 Translating...\n⏱️  'How is the weather today...' -> Japanese in 1.04s\n\n✅ Result in Japanese:\n今日は天気怎么样ですか。\n---------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n🇬🇧 English text (or 'quit' to exit):  How is the weather today\n🌍 Target language for 'How is the weather today...':  Chinese\n"},{"name":"stdout","text":"\n🔄 Translating...\n⏱️  'How is the weather today...' -> Chinese in 0.79s\n\n✅ Result in Chinese:\n今天天气怎么样？\n---------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n🇬🇧 English text (or 'quit' to exit):  quit\n"},{"name":"stdout","text":"\nExiting translator. Goodbye!\n","output_type":"stream"}],"execution_count":3}]}