{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":397953,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":326098,"modelId":317146}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. SETUP AND INSTALLATION\n# -------------------------\nprint(\"Installing required packages (transformers from source)...\")\n!pip install -q pandas --upgrade\n!pip install -q accelerate\n!pip install -q git+https://github.com/huggingface/transformers.git\n!pip install -q sentencepiece\n\nimport os\nimport time\nimport torch\nimport kagglehub\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom typing import List\n\nprint(\"âœ… Packages installed successfully!\")\n\n\n# 2. MODEL CONFIGURATION\n# ----------------------\nprint(\"Downloading generative model files with kagglehub...\")\nMODEL_PATH = kagglehub.model_download(\"google/gemma-3n/transformers/gemma-3n-e2b-it\")\nprint(f\"âœ… Model path set to: {MODEL_PATH}\")\n\n\n# 3. GEMMA GENERATIVE TRANSLATOR CLASS\n# ------------------------------------\nclass GemmaTranslator:\n    def __init__(self, model_path: str):\n        \"\"\"Initializes the translator by loading the generative model with Transformers.\"\"\"\n        self.model_path = model_path\n        self.tokenizer = None\n        self.model = None\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self._initialize_model()\n\n    def _initialize_model(self):\n        \"\"\"Loads the Gemma model using the AutoModelForCausalLM interface.\"\"\"\n        if not os.path.exists(self.model_path):\n            print(f\"âŒ Cannot initialize model: Path '{self.model_path}' does not exist.\")\n            return\n\n        try:\n            print(\"ğŸ”„ Loading tokenizer...\")\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n\n            print(\"ğŸ”„ Loading model... (This may take a moment)\")\n            self.model = AutoModelForCausalLM.from_pretrained(\n                self.model_path,\n                torch_dtype=torch.float16,\n            ).to(self.device)\n\n            print(\"âœ… Translator initialized successfully!\")\n            if self.model:\n                print(f\"ğŸ“Š Model loaded on: {self.model.device}\")\n\n        except Exception as e:\n            print(f\"âŒ Error initializing model: {e}\")\n            self.model = None\n            self.tokenizer = None\n\n    def translate(self, text: str, target_language: str = \"Italian\", source_language: str = \"English\") -> str:\n        \"\"\"\n        Translates text by generating a translation from scratch.\n        No phrasebook is used.\n        \"\"\"\n        if not self.model or not self.tokenizer:\n            return \"âŒ Model not initialized. Please check for errors above.\"\n\n        # Create a dynamic prompt for the instruction-tuned model.\n        prompt = f\"\"\"Translate the following {source_language} text to {target_language}. Provide only the {target_language} translation.\n\n{source_language}: \"{text}\"\n{target_language}:\"\"\"\n\n        try:\n            start_time = time.time()\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n\n            # Use disable_compile=True for compatibility with older Kaggle GPUs (like P100)\n            outputs = self.model.generate(**inputs, max_new_tokens=150, disable_compile=True)\n\n            # Decode and clean the output\n            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            end_time = time.time()\n\n            # Extract only the clean translation from the full response.\n            if f\"{target_language}:\" in response:\n                translation = response.split(f\"{target_language}:\")[-1].strip()\n            else:\n                # Fallback if the model doesn't follow the format perfectly.\n                translation = response[len(prompt) - len(f'\\n{target_language}:'):].strip()\n\n            # Final cleanup to remove any stray quotation marks.\n            translation = translation.replace('\"', '').strip()\n\n            print(f\"â±ï¸  '{text[:30]}...' -> {target_language} in {end_time - start_time:.2f}s\")\n            return translation\n\n        except Exception as e:\n            return f\"âŒ Translation error: {e}\"\n\n    def batch_translate(self, texts: List[str], target_language: str = \"Italian\") -> List[str]:\n        \"\"\"Translates a list of texts.\"\"\"\n        return [self.translate(text, target_language) for text in texts]\n\n\n# 4. INITIALIZE AND TEST THE TRANSLATOR\n# -------------------------------------\ntranslator = GemmaTranslator(MODEL_PATH)\n\nif translator.model:\n    # Supported languages for testing\n    SUPPORTED_LANGUAGES = {\n        \"Italian\": \"Italiano\",\n        \"Arabic\": \"Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\",\n        \"Chinese\": \"ä¸­æ–‡\",\n        \"Spanish\": \"EspaÃ±ol\",\n        \"French\": \"FranÃ§ais\",\n        \"German\": \"Deutsch\",\n        \"Japanese\": \"æ—¥æœ¬èª\",\n    }\n    print(\"\\nğŸŒ Supported Test Languages:\", \", \".join(SUPPORTED_LANGUAGES.keys()))\n\n    print(\"\\nğŸ”„ Starting Translation Examples...\")\n    print(\"=\" * 60)\n\n    # Translate to various languages\n    test_sentence = \"Where is the nearest restaurant?\"\n    for lang_code, lang_name in SUPPORTED_LANGUAGES.items():\n        print(f\"\\nğŸ‡ºğŸ‡¸ English to {lang_name} ({lang_code})\")\n        print(\"-\" * 30)\n        translation = translator.translate(test_sentence, lang_code)\n        print(f\"EN: {test_sentence}\")\n        print(f\"{lang_code[:2].upper()}: {translation}\")\n\n\n# 5. PERFORMANCE ANALYSIS\n# -----------------------\ndef analyze_performance():\n    if not translator.model:\n        print(\"âŒ Cannot analyze performance, model not loaded.\")\n        return\n\n    test_text = \"Can you recommend a good local hotel?\"\n    results = []\n\n    print(\"\\n\\nğŸ“Š Performance Analysis\")\n    print(\"=\" * 40)\n    print(f\"Testing with sentence: \\\"{test_text}\\\"\")\n\n    for language in [\"French\", \"Spanish\", \"German\", \"Japanese\"]:\n        translation = translator.translate(test_text, language)\n        # We can't get the time from the translate function directly anymore, so we re-time here for the analysis\n        start_time = time.time()\n        _ = translator.translate(test_text, language) # Run again just for timing\n        end_time = time.time()\n\n        results.append({\n            'Language': language,\n            'Translation': translation,\n            'Time (seconds)': round(end_time - start_time, 2)\n        })\n\n    df = pd.DataFrame(results)\n    print(\"\\n--- Performance Results ---\")\n    print(df.to_string(index=False))\n    print(\"\\n--- Summary ---\")\n    print(f\"ğŸ“Š Average translation time: {df['Time (seconds)'].mean():.2f} seconds\")\n    print(f\"ğŸ“Š Fastest translation: {df.loc[df['Time (seconds)'].idxmin(), 'Language']} ({df['Time (seconds)'].min():.2f}s)\")\n    print(f\"ğŸ“Š Slowest translation: {df.loc[df['Time (seconds)'].idxmax(), 'Language']} ({df['Time (seconds)'].max():.2f}s)\")\n    return df\n\nperformance_df = analyze_performance()\n\n\n# 6. INTERACTIVE TRANSLATOR\n# -------------------------\ndef interactive_translator():\n    if not translator.model:\n        print(\"âŒ Cannot start interactive mode, model not loaded.\")\n        return\n\n    print(\"\\n\\nğŸŒ Gemma Generative Interactive Translator\")\n    print(\"=\" * 40)\n    print(\"Available languages:\", \", \".join(SUPPORTED_LANGUAGES.keys()))\n    print(\"Type 'quit' to exit\\n\")\n\n    while True:\n        text = input(\"ğŸ“ Enter text to translate (English): \")\n        if text.lower() == 'quit':\n            break\n\n        target_lang = input(f\"ğŸ¯ Target language ({'/'.join(SUPPORTED_LANGUAGES.keys())}): \")\n        if target_lang not in SUPPORTED_LANGUAGES:\n            print(f\"âŒ Language '{target_lang}' not in test list. Defaulting to Italian.\")\n            target_lang = \"Italian\"\n\n        translation = translator.translate(text, target_lang)\n        print(f\"âœ… Translation: {translation}\")\n        print(\"-\" * 50)\n\n# Uncomment the line below to run the interactive translator\n# interactive_translator()\n\n\n# 7. MODEL INFORMATION\n# --------------------\ndef display_model_info():\n    print(\"\\n\\nğŸ¤– GEMMA 3N MODEL INFORMATION\")\n    print(\"=\" * 50)\n    print(\"ğŸ“Š Model Used: google/gemma-3n/transformers/gemma-3n-e2b-it\")\n    print(\"ğŸ“Š Architecture: Gemma 3n-E2B-IT (Instruction Tuned Generative Model)\")\n    print(\"ğŸ“Š Framework: PyTorch / Transformers\")\n    print(f\"ğŸ“Š Inference Device: {translator.device.upper()}\")\n    print(\"ğŸ“Š Key Feature: Generates translations for arbitrary text, no phrasebook required.\")\n    print(\"\\nğŸš€ FIX APPLIED:\")\n    print(\"â€¢ `disable_compile=True` used in `model.generate()` to ensure compatibility on all GPUs.\")\n\ndisplay_model_info()\n\n# Verify environment and GPU usage\nprint(\"\\nğŸ” Verifying GPU Usage\")\n!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}