{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":397953,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":326098,"modelId":317146}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. SETUP AND INSTALLATION\n# -------------------------\nprint(\"Installing required packages (transformers from source)...\")\n!pip install -q pandas --upgrade\n!pip install -q accelerate\n!pip install -q git+https://github.com/huggingface/transformers.git\n!pip install -q sentencepiece\n\nimport os\nimport time\nimport torch\nimport kagglehub\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom typing import List\n\nprint(\"✅ Packages installed successfully!\")\n\n\n# 2. MODEL CONFIGURATION\n# ----------------------\nprint(\"Downloading generative model files with kagglehub...\")\nMODEL_PATH = kagglehub.model_download(\"google/gemma-3n/transformers/gemma-3n-e2b-it\")\nprint(f\"✅ Model path set to: {MODEL_PATH}\")\n\n\n# 3. GEMMA GENERATIVE TRANSLATOR CLASS\n# ------------------------------------\nclass GemmaTranslator:\n    def __init__(self, model_path: str):\n        \"\"\"Initializes the translator by loading the generative model with Transformers.\"\"\"\n        self.model_path = model_path\n        self.tokenizer = None\n        self.model = None\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self._initialize_model()\n\n    def _initialize_model(self):\n        \"\"\"Loads the Gemma model using the AutoModelForCausalLM interface.\"\"\"\n        if not os.path.exists(self.model_path):\n            print(f\"❌ Cannot initialize model: Path '{self.model_path}' does not exist.\")\n            return\n\n        try:\n            print(\"🔄 Loading tokenizer...\")\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n\n            print(\"🔄 Loading model... (This may take a moment)\")\n            self.model = AutoModelForCausalLM.from_pretrained(\n                self.model_path,\n                torch_dtype=torch.float16,\n            ).to(self.device)\n\n            print(\"✅ Translator initialized successfully!\")\n            if self.model:\n                print(f\"📊 Model loaded on: {self.model.device}\")\n\n        except Exception as e:\n            print(f\"❌ Error initializing model: {e}\")\n            self.model = None\n            self.tokenizer = None\n\n    def translate(self, text: str, target_language: str = \"Italian\", source_language: str = \"English\") -> str:\n        \"\"\"\n        Translates text by generating a translation from scratch.\n        No phrasebook is used.\n        \"\"\"\n        if not self.model or not self.tokenizer:\n            return \"❌ Model not initialized. Please check for errors above.\"\n\n        # Create a dynamic prompt for the instruction-tuned model.\n        prompt = f\"\"\"Translate the following {source_language} text to {target_language}. Provide only the {target_language} translation.\n\n{source_language}: \"{text}\"\n{target_language}:\"\"\"\n\n        try:\n            start_time = time.time()\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n\n            # Use disable_compile=True for compatibility with older Kaggle GPUs (like P100)\n            outputs = self.model.generate(**inputs, max_new_tokens=150, disable_compile=True)\n\n            # Decode and clean the output\n            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            end_time = time.time()\n\n            # Extract only the clean translation from the full response.\n            if f\"{target_language}:\" in response:\n                translation = response.split(f\"{target_language}:\")[-1].strip()\n            else:\n                # Fallback if the model doesn't follow the format perfectly.\n                translation = response[len(prompt) - len(f'\\n{target_language}:'):].strip()\n\n            # Final cleanup to remove any stray quotation marks.\n            translation = translation.replace('\"', '').strip()\n\n            print(f\"⏱️  '{text[:30]}...' -> {target_language} in {end_time - start_time:.2f}s\")\n            return translation\n\n        except Exception as e:\n            return f\"❌ Translation error: {e}\"\n\n    def batch_translate(self, texts: List[str], target_language: str = \"Italian\") -> List[str]:\n        \"\"\"Translates a list of texts.\"\"\"\n        return [self.translate(text, target_language) for text in texts]\n\n\n# 4. INITIALIZE AND TEST THE TRANSLATOR\n# -------------------------------------\ntranslator = GemmaTranslator(MODEL_PATH)\n\nif translator.model:\n    # Supported languages for testing\n    SUPPORTED_LANGUAGES = {\n        \"Italian\": \"Italiano\",\n        \"Arabic\": \"العربية\",\n        \"Chinese\": \"中文\",\n        \"Spanish\": \"Español\",\n        \"French\": \"Français\",\n        \"German\": \"Deutsch\",\n        \"Japanese\": \"日本語\",\n    }\n    print(\"\\n🌍 Supported Test Languages:\", \", \".join(SUPPORTED_LANGUAGES.keys()))\n\n    print(\"\\n🔄 Starting Translation Examples...\")\n    print(\"=\" * 60)\n\n    # Translate to various languages\n    test_sentence = \"Where is the nearest restaurant?\"\n    for lang_code, lang_name in SUPPORTED_LANGUAGES.items():\n        print(f\"\\n🇺🇸 English to {lang_name} ({lang_code})\")\n        print(\"-\" * 30)\n        translation = translator.translate(test_sentence, lang_code)\n        print(f\"EN: {test_sentence}\")\n        print(f\"{lang_code[:2].upper()}: {translation}\")\n\n\n# 5. PERFORMANCE ANALYSIS\n# -----------------------\ndef analyze_performance():\n    if not translator.model:\n        print(\"❌ Cannot analyze performance, model not loaded.\")\n        return\n\n    test_text = \"Can you recommend a good local hotel?\"\n    results = []\n\n    print(\"\\n\\n📊 Performance Analysis\")\n    print(\"=\" * 40)\n    print(f\"Testing with sentence: \\\"{test_text}\\\"\")\n\n    for language in [\"French\", \"Spanish\", \"German\", \"Japanese\"]:\n        translation = translator.translate(test_text, language)\n        # We can't get the time from the translate function directly anymore, so we re-time here for the analysis\n        start_time = time.time()\n        _ = translator.translate(test_text, language) # Run again just for timing\n        end_time = time.time()\n\n        results.append({\n            'Language': language,\n            'Translation': translation,\n            'Time (seconds)': round(end_time - start_time, 2)\n        })\n\n    df = pd.DataFrame(results)\n    print(\"\\n--- Performance Results ---\")\n    print(df.to_string(index=False))\n    print(\"\\n--- Summary ---\")\n    print(f\"📊 Average translation time: {df['Time (seconds)'].mean():.2f} seconds\")\n    print(f\"📊 Fastest translation: {df.loc[df['Time (seconds)'].idxmin(), 'Language']} ({df['Time (seconds)'].min():.2f}s)\")\n    print(f\"📊 Slowest translation: {df.loc[df['Time (seconds)'].idxmax(), 'Language']} ({df['Time (seconds)'].max():.2f}s)\")\n    return df\n\nperformance_df = analyze_performance()\n\n\n# 6. INTERACTIVE TRANSLATOR\n# -------------------------\ndef interactive_translator():\n    if not translator.model:\n        print(\"❌ Cannot start interactive mode, model not loaded.\")\n        return\n\n    print(\"\\n\\n🌍 Gemma Generative Interactive Translator\")\n    print(\"=\" * 40)\n    print(\"Available languages:\", \", \".join(SUPPORTED_LANGUAGES.keys()))\n    print(\"Type 'quit' to exit\\n\")\n\n    while True:\n        text = input(\"📝 Enter text to translate (English): \")\n        if text.lower() == 'quit':\n            break\n\n        target_lang = input(f\"🎯 Target language ({'/'.join(SUPPORTED_LANGUAGES.keys())}): \")\n        if target_lang not in SUPPORTED_LANGUAGES:\n            print(f\"❌ Language '{target_lang}' not in test list. Defaulting to Italian.\")\n            target_lang = \"Italian\"\n\n        translation = translator.translate(text, target_lang)\n        print(f\"✅ Translation: {translation}\")\n        print(\"-\" * 50)\n\n# Uncomment the line below to run the interactive translator\n# interactive_translator()\n\n\n# 7. MODEL INFORMATION\n# --------------------\ndef display_model_info():\n    print(\"\\n\\n🤖 GEMMA 3N MODEL INFORMATION\")\n    print(\"=\" * 50)\n    print(\"📊 Model Used: google/gemma-3n/transformers/gemma-3n-e2b-it\")\n    print(\"📊 Architecture: Gemma 3n-E2B-IT (Instruction Tuned Generative Model)\")\n    print(\"📊 Framework: PyTorch / Transformers\")\n    print(f\"📊 Inference Device: {translator.device.upper()}\")\n    print(\"📊 Key Feature: Generates translations for arbitrary text, no phrasebook required.\")\n    print(\"\\n🚀 FIX APPLIED:\")\n    print(\"• `disable_compile=True` used in `model.generate()` to ensure compatibility on all GPUs.\")\n\ndisplay_model_info()\n\n# Verify environment and GPU usage\nprint(\"\\n🔍 Verifying GPU Usage\")\n!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T05:26:36.050004Z","iopub.execute_input":"2025-06-28T05:26:36.050263Z","iopub.status.idle":"2025-06-28T05:29:36.005820Z","shell.execute_reply.started":"2025-06-28T05:26:36.050243Z","shell.execute_reply":"2025-06-28T05:29:36.005018Z"}},"outputs":[{"name":"stdout","text":"Installing required packages (transformers from source)...\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n✅ Packages installed successfully!\nDownloading generative model files with kagglehub...\n✅ Model path set to: /kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1\n🔄 Loading tokenizer...\n🔄 Loading model... (This may take a moment)\n","output_type":"stream"},{"name":"stderr","text":"2025-06-28 05:29:24.483628: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751088564.671717      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751088564.729460      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"❌ Error initializing model: Unknown model (mobilenetv5_300m_enc)\n❌ Cannot analyze performance, model not loaded.\n\n\n🤖 GEMMA 3N MODEL INFORMATION\n==================================================\n📊 Model Used: google/gemma-3n/transformers/gemma-3n-e2b-it\n📊 Architecture: Gemma 3n-E2B-IT (Instruction Tuned Generative Model)\n📊 Framework: PyTorch / Transformers\n📊 Inference Device: CUDA\n📊 Key Feature: Generates translations for arbitrary text, no phrasebook required.\n\n🚀 FIX APPLIED:\n• `disable_compile=True` used in `model.generate()` to ensure compatibility on all GPUs.\n\n🔍 Verifying GPU Usage\nSat Jun 28 05:29:35 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0             26W /  250W |       3MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}