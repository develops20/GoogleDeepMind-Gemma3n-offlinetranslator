{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. SETUP AND INSTALLATION\n# -------------------------\n# Using the exact setup from your working script.\nprint(\"Installing required packages (transformers from source)...\")\n!pip install -q timm --upgrade\n!pip install -q accelerate\n!pip install -q git+https://github.com/huggingface/transformers.git\n\nimport os\nimport time\nimport torch\nimport kagglehub\n# FIX: Reverting to AutoModelForCausalLM as per your working example\nfrom transformers import AutoTokenizer, AutoModelForCausalLM \nfrom typing import List\n\n# These environment settings are from your working script.\n# While `disable_compile=True` is the main fix used later, we will keep these.\nprint(\"🔄 Configuring PyTorch backend for compatibility...\")\ntry:\n    import torch._dynamo\n    torch._dynamo.reset() \n    os.environ[\"TORCH_COMPILE_BACKEND\"] = \"eager\"\n    torch._dynamo.config.suppress_errors = True\nexcept ImportError:\n    print(\"⚠️ Could not import torch._dynamo. Skipping dynamo configuration.\")\n\nprint(\"✅ Packages installed successfully!\")\n\n\n# 2. MODEL CONFIGURATION\n# ----------------------\nprint(\"Downloading model files with kagglehub...\")\nMODEL_PATH = kagglehub.model_download(\"google/gemma-3n/transformers/gemma-3n-e2b-it\")\nprint(f\"✅ Model path set to: {MODEL_PATH}\")\n\n\n# 3. GENERAL-PURPOSE TRANSLATOR CLASS\n# -----------------------------------\n# This class is based directly on your working EnglishToItalianTranslator.\n# It has been generalized to handle multiple languages.\nclass GemmaTranslator: # CHANGED: Renamed class\n    def __init__(self, model_path):\n        self.model_path = model_path\n        self.tokenizer = None\n        self.model = None\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self._initialize_model()\n\n    def _initialize_model(self):\n        \"\"\"Loads the Gemma model using AutoModelForCausalLM.\"\"\"\n        if not os.path.exists(self.model_path):\n            print(\"❌ Cannot initialize model: Path does not exist.\")\n            return\n\n        try:\n            print(\"🔄 Loading tokenizer...\")\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n            \n            print(\"🔄 Loading model... (This may take a moment)\")\n            # FIX: Using AutoModelForCausalLM as it correctly handles the 'gemma3n' model type.\n            self.model = AutoModelForCausalLM.from_pretrained(\n                self.model_path,\n                torch_dtype=torch.float16,\n            ).to(self.device)\n\n            print(\"✅ Translator initialized successfully!\")\n            if self.model:\n                  print(f\"📊 Model configured for device: {self.device}\")\n\n        except Exception as e:\n            print(f\"❌ Error initializing model: {e}\")\n            self.model = None\n            self.tokenizer = None\n            \n    # CHANGED: This method now accepts a `target_language` argument.\n    def translate(self, english_text: str, target_language: str) -> str:\n        \"\"\"\n        Translates English text into the specified target language.\n        \"\"\"\n        if not self.model or not self.tokenizer:\n            return \"❌ Model not initialized. Please check for errors above.\"\n\n        # The prompt is now dynamic based on the target_language.\n        prompt = f\"\"\"Translate the following English text to {target_language}. Provide only the {target_language} translation.\n\nEnglish: \"{english_text}\"\n{target_language}:\"\"\"\n\n        try:\n            start_time = time.time()\n            \n            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n            \n            # This is the key fix for GPU compatibility.\n            outputs = self.model.generate(**inputs, max_new_tokens=100, disable_compile=True)\n            \n            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            end_time = time.time()\n            \n            # The response parsing is now also dynamic.\n            if f\"{target_language}:\" in response:\n                translation = response.split(f\"{target_language}:\")[-1].strip()\n            else:\n                translation = response[len(prompt) - len(f'\\n{target_language}:'):].strip()\n            \n            translation = translation.replace('\"', '').strip()\n            \n            print(f\"⏱️  '{english_text[:30]}...' -> {target_language} in {end_time - start_time:.2f}s\")\n            return translation\n\n        except Exception as e:\n            return f\"❌ Translation error: {e}\"\n\n\n# 4. INITIALIZE AND TEST THE MULTILINGUAL TRANSLATOR\n# ----------------------------------------------------\nprint(\"\\n--- Initializing Translator ---\")\ntranslator = GemmaTranslator(MODEL_PATH)\nprint(\"-----------------------------\\n\")\n\nif translator.model:\n    # A dictionary of sentences to test the new multilingual capability.\n    test_cases = {\n        \"Where is the nearest library?\": \"Spanish\",\n        \"How much does this cost?\": \"German\",\n        \"I would like to order a coffee.\": \"French\",\n        \"This is a very powerful language model.\": \"Italian\",\n    }\n\n    print(\"--- Running Multilingual Translation Examples ---\")\n    for sentence, language in test_cases.items():\n        translation = translator.translate(sentence, language)\n        print(f\"🇺🇸 English: {sentence}\")\n        print(f\"🌍 {language}: {translation}\")\n        print(\"-\" * 40)\n        \n    print(\"\\n🎉 All tasks complete!\")\nelse:\n    print(\"Could not run translation examples because the model failed to initialize.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T06:00:40.955212Z","iopub.execute_input":"2025-06-28T06:00:40.955541Z","iopub.status.idle":"2025-06-28T06:02:25.299200Z","shell.execute_reply.started":"2025-06-28T06:00:40.955518Z","shell.execute_reply":"2025-06-28T06:02:25.298248Z"}},"outputs":[{"name":"stdout","text":"Installing required packages (transformers from source)...\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n🔄 Configuring PyTorch backend for compatibility...\n✅ Packages installed successfully!\nDownloading model files with kagglehub...\n✅ Model path set to: /kaggle/input/gemma-3n/transformers/gemma-3n-e2b-it/1\n\n--- Initializing Translator ---\n🔄 Loading tokenizer...\n🔄 Loading model... (This may take a moment)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96cf472a1e5c43c09176079d057a5c3b"}},"metadata":{}},{"name":"stdout","text":"✅ Translator initialized successfully!\n📊 Model configured for device: cuda\n-----------------------------\n\n--- Running Multilingual Translation Examples ---\n⏱️  'Where is the nearest library?...' -> Spanish in 2.59s\n🇺🇸 English: Where is the nearest library?\n🌍 Spanish: ¿Dónde está la biblioteca más cercana?\n----------------------------------------\n⏱️  'How much does this cost?...' -> German in 13.54s\n🇺🇸 English: How much does this cost?\n🌍 German: Ent\n----------------------------------------\n⏱️  'I would like to order a coffee...' -> French in 1.23s\n🇺🇸 English: I would like to order a coffee.\n🌍 French: Je voudrais commander un café.\n----------------------------------------\n⏱️  'This is a very powerful langua...' -> Italian in 1.47s\n🇺🇸 English: This is a very powerful language model.\n🌍 Italian: Questo è un modello linguistico molto potente.\n----------------------------------------\n\n🎉 All tasks complete!\n","output_type":"stream"}],"execution_count":2}]}